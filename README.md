### ðŸ”§ Built With

![Python](https://img.shields.io/badge/Python-3.7%2B-blue)
![NLP](https://img.shields.io/badge/NLP-Text%20Classification-green)
![Model](https://img.shields.io/badge/Model-TF--IDF%20%2B%20LogReg-orange)
![Backend](https://img.shields.io/badge/Backend-Flask-black)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

# ðŸ“š Amazon Data Science Book Classification

A complete end-to-end NLP machine learning project that classifies Amazon Data Science books using TF-IDF features and Logistic Regression, wrapped inside a production-ready pipeline with training, evaluation, and a Flask web application for real-time predictions.

---

## ðŸš€ Project Overview

This project is a full-stack Natural Language Processing (NLP) machine learning application designed to automatically classify Amazon Data Scienceâ€“related books based on textual information such as book titles or descriptions.

The goal of the project is to demonstrate how a real-world text classification system can be built using classical machine learning techniques while following production-level software engineering practices. Rather than focusing only on model accuracy, the project emphasizes clean architecture, modular design, reproducibility, and deployability, which are critical in industry environments.

At its core, the system converts raw text into numerical representations using TF-IDF (Term Frequencyâ€“Inverse Document Frequency), capturing both the importance of words and their contextual relevance. These features are then fed into a Logistic Regression classifier, a strong and interpretable baseline model widely used for large-scale text classification tasks.

The entire workflow is structured as a pipeline consisting of clearly separated stages:

- Data ingestion from a structured CSV source

- Model training using configurable hyperparameters

- Model evaluation with comprehensive performance metrics

- Real-time inference through a RESTful API

To make the project end-user accessible, a Flask web application is integrated, allowing users to input text and receive predictions instantly. All intermediate outputsâ€”including trained models, logs, and evaluation reportsâ€”are stored as versioned artifacts, ensuring traceability and reproducibility.

---

## Tech Stack
- Programming Language: Python

- Machine Learning:

- TF-IDF Vectorizer

- Logistic Regression (Scikit-learn)

- Backend: Flask

- Pipeline Management: Custom modular pipeline

- Model Persistence: Joblib

- Configuration: YAML

- Frontend: HTML + Bootstrap

- Deployment Ready: Yes 

---

## ðŸ“‚ Project Structure

<pre>
Amazon-Data-Science-Book/
â”‚
â”œâ”€â”€ app.py                         # Flask application
â”œâ”€â”€ main.py                        # Pipeline runner
â”œâ”€â”€ setup.py                       # Package setup
â”œâ”€â”€ params.yaml                    # Model & training parameters
â”œâ”€â”€ config.yaml                    # Pipeline configuration
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ data_ingestion/            # Raw & processed data
â”‚   â”œâ”€â”€ training/                  # Trained model
â”‚   â””â”€â”€ evaluation/                # Evaluation reports
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ cnnClassifier/
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ data_ingestion.py
â”‚       â”‚   â”œâ”€â”€ training.py
â”‚       â”‚   â”œâ”€â”€ evaluation.py
â”‚       â”‚   â””â”€â”€ prediction.py
â”‚       â”‚
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â””â”€â”€ configuration.py
â”‚       â”‚
â”‚       â”œâ”€â”€ entity/
â”‚       â”‚   â””â”€â”€ config_entity.py
â”‚       â”‚
â”‚       â”œâ”€â”€ pipeline/
â”‚       â”‚   â”œâ”€â”€ stage_01_data_ingestion.py
â”‚       â”‚   â”œâ”€â”€ stage_03_training.py
â”‚       â”‚   â””â”€â”€ stage_04_evaluation.py
â”‚       â”‚
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â””â”€â”€ common.py
â”‚       â”‚
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                 # Web UI
â”‚
â””â”€â”€ logs/
    â””â”€â”€ running_logs.log

</pre>

---

## DVC Pipeline Stages

1. Data Ingestion  
2. Model Training  
3. Model Evaluation  

Run the full pipeline:
<pre>dvc repro</pre>

## Clone the Repository

```text
git clone https://github.com/Arnab-Ghosh7/Amazon-Data-Science-Book-Predication.git
cd Amazon-Data-Science-Book-Predication
```
## Create Environment & Install Dependencies

```text
conda create -n nlp_env python=3.10 -y
conda activate nlp_env
pip install -r requirements.txt
```

### Train the Model
```
python main.py
```

### Run Flask App
```
python app.py
```
### Open in browser:
```
http://127.0.0.1:5000/
```

---

## Model Output

The model predicts whether the chicken fecal image belongs to:
- Accuracy

- Precision (weighted)

- Recall (weighted)

- F1-Score

- Full classification report

Evaluation metrics are stored in - artifacts/evaluation/



---

## Author

Arnab Ghosh  

GitHub: https://github.com/Arnab-Ghosh7

---

## License

This project is licensed under the MIT License.


